---
title: "Case Study: MovieMagic Concessions and User Review Statistical Analysis"
author: "Brian Burdick, Chris Gearheart and Chris Porter"

output:
  html_document: default
  pdf_document: default
---
```{r setup, include= FALSE}
# Install Libraries
library(dplyr)
library(psycho)
library(car)
#library(caret)
library(knitr)

# Load project data
data <- read.csv("http://data.mishra.us/files/project_data.csv")
#datasplit <- createDataPartition(data$amount_spent, p = 0.7, list=FALSE)
#trainData <- data[datasplit,]
#testData <- data[-datasplit,]
#text <- read.csv(url("http://data.mishra.us/files/project_reviews.csv"))
```

##  Introduction

*MovieMagic*, is a regional movie chain operating in the Southwest region of the US.  They are considering ways in which they can
increase spending on concessions. MovieMagic has collected information about 2000 of its customers, some of whom are part of their loyalty program and some who are not. They have information on 8 variables, which they plan to use as predictors. They plan to use the amount spent on concessions as the outcome variable
since they have learned from observation that much of their profit is derived from concession sales.

**Regression Analysis**


```{r linear regression, echo= FALSE, warning=FALSE, message=FALSE}
#head(data)
model1<- lm(amount_spent~., data=data)
summary(model1) # will give output for each level of each categorical predictor

#model2 <- aov(amount_spent~., data = data)
#summary(model2)

```
Based upon linear regression analysis, we are able to identify age, streaming, days_member and movies_seen as the significant predictors (p <0.05) of amount spent on concessions.  <!--Upon eliminating non-significant predictor variables, we see that the model continues to explain a similar amount of the variation (MRS 0.6738 -> 0.635), so little loss of information has occurred. -->

```{r LM refinement, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
modelLM<- lm(amount_spent~age+streaming+days_member+movies_seen, data= data)
summary(modelLM)

```
Of the signficant independent variables, *age*, *days_member* and *movies_seen* all display a positive influence on the outcome variable *amount_spent* meaning that for every increase in these variables, it is predicted that the amount spent on concession will also increase.  The only significant predictor variable to display a negative influence is *streaming* which shows a remarkably strong negative influence when compared with the other predictor variables.  Therefore any increase in this variable will strongly decrease the *amount_spent* outcome variable.  This was determined using a linear regression. To determine whether a penalized regression analysis would further inform this, we need to determine to what extent the independent variables are correlated.  As with any data set of seemingly independent variables, the possibility exists that the independent variables are not in fact independent.  To determine this we check for multicollinearity.

*Multicollinear Analysis*

The primary test for mulitcollinearity is to calculate the Variance Inflation Factor (VIF) for each independent variable.

```{r VIF, echo=FALSE, warning=FALSE, message=FALSE}
vif(model1)
#vif(modelLM)
```
None of the variables display a high degree of multicollinearity (VIF > 5).  As such, no measures are required to address multicollinearity in the model.

**Penalized Regression Analysis**

```{r, warning=FALSE, message=FALSE}


```


**Predictive model**
The analysis was run by splitting the data........
```{r, warning=FALSE, message=FALSE}


```

**Text Analysis**

Question 6
MovieMagic wants to visualize the reviews through a wordcloud and wants to find
out which words are used most commonly in the reviews that customers write for
MovieMagic. Create 2 wordclouds - one for reviews that received 3, 4, or 5 star
ratings and another with reviews that received 1 or 2 stars ratings. Knowing the
prominent words in each of the wordclouds, what strategies can be developed in
messaging customers? Would the strategies differ?

Answer: 
"The strategies will differ greatly because the negative group is damage control messaging and the 
positive group is reminding them of why they loved going to the movie.  

Positive Reviews
The three most common words are Great, Movie, and Food.  Based on the top three most common words 
customers enjoyed the movies and food that was offered.  A few strategies that we would suggest 
focusing on are. Communication around new movies coming out and seasonal concession treats with images. 
Ideally if name and card information could be tied to what they purchased at the concession stand maybe 
a bogo could be offered to this group.  We would also like to conduct additional analysis to see what 
messaging is more effective i.e. new movie + concession bogo, new movie, new movie + discounted popcorn 
etc.  We would want to get this dialed in to see which is more effective.

Negative Reviews
The three most common words are Hour, Food, and Like.  Based on the word cloud the service was slow 
and took around an hour to get their food.  The negative reviews sound like they need internal strategies 
more than external ones.  Perhaps the theater concessions are under staffed, poorly organized, or has 
inefficient processes.  This could be focus area for the theaters manager to improve for their customers.
A time study could be completed on various nights to see the length of time it takes for people to get 
their food.  Once that number has been improved messaging to this group could be around the improvements 
in the speed of service. "

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(topicmodels)
library(ggplot2)
library(dplyr)
library(tidytext)
library(tidyr)
library(RTextTools)
library(wordcloud)
library(tm)
library(stringr)

reviews <- read.csv(url("http://data.mishra.us/files/project_reviews.csv"))

#Positive Word Cloud

Positive_Reviews <- filter(reviews,star >= 3)

Review_corpus_p <- VCorpus(VectorSource(Positive_Reviews$text))

Review_corpus_p <- tm_map(Review_corpus_p, content_transformer(tolower)) # covert all to lower case else same word as lower and uppercase will classified as different
Review_corpus_p <- tm_map(Review_corpus_p, removeWords, stopwords(kind="en")) # remove stop words
func_Space <- content_transformer(function(x, pattern) gsub(pattern, " ", x)) # a function to clean /,@,\\,|
Review_corpus_p <- tm_map(Review_corpus_p, func_Space, "/|@|\\|")
Review_corpus_p <- tm_map(Review_corpus_p, stripWhitespace) # remove white space
Review_corpus_p <- tm_map(Review_corpus_p, removeNumbers) # remove numeric values
Review_corpus_p <- tm_map(Review_corpus_p, removePunctuation) # remove punctuations

dtm_review <- TermDocumentMatrix(Review_corpus_p)

m <- as.matrix(dtm_review)
v <- sort(rowSums(m),decreasing=TRUE)
review_final <- data.frame(word = names(v),freq=v)

set.seed(1234)
wordcloud(words = review_final$word, freq = review_final$freq, min.freq = 2,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"),scale=c(3, 0.7))

##Negative Word Cloud

Negative_Reviews <- filter(reviews,star < 3)

Review_corpus_n <- VCorpus(VectorSource(Negative_Reviews$text))

Review_corpus_n <- tm_map(Review_corpus_n, content_transformer(tolower)) # covert all to lower case else same word as lower and uppercase will classified as different

Review_corpus_n <- tm_map(Review_corpus_n, removeWords, stopwords(kind="en")) # if we remove stopwords the wordcloud becomes sparse. 
# Run with and without stop words to see how word cloud changes

func_Space <- content_transformer(function(x, pattern) gsub(pattern, " ", x)) # a function to clean /,@,\\,|
Review_corpus_n <- tm_map(Review_corpus_n, func_Space, "/|@|\\|")
Review_corpus_n <- tm_map(Review_corpus_n, stripWhitespace) # remove white space
Review_corpus_n <- tm_map(Review_corpus_n, removeNumbers) # remove numbers
Review_corpus_n <- tm_map(Review_corpus_n, removePunctuation) # remove punctuations


dtm_review_n <- TermDocumentMatrix(Review_corpus_n)

m_n <- as.matrix(dtm_review_n )
v_n <- sort(rowSums(m_n),decreasing=TRUE)
review_final_n <- data.frame(word = names(v_n),freq=v_n)

set.seed(1234)
wordcloud(words = review_final_n$word, freq = review_final_n$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"),scale=c(3, .5))
```

7. MovieMagic also wants to use topic modeling to find out whether the content in
the reviews could be categorized into specific topics. If you used LDA to create 3 topic groups (k = 3), MovieMagic wants you to use 
the words within the 3 topics to infer topic title.  Which term is the most relevant in each of the three topics and how would it inform your 
business strategy? Given the topics you inferred what strategies would you suggest are possible for MovieMagic if it wants to increase con-
cession sales. Would you recommend promotions or advertising or loyalty program; justify your choice of business strategy?

Answer

Topic 1 
Title: Concessions
Top Word: Food
Top 3 words: Food, Fun, Cinema
Strategy: We would create a loyalty program so that we could gain additional insight into our customer base.  Once we have a large enough sample, we would utilize a clustering technique to identify common characteristics across our most lucrative segments and then cater promotions and advertising to them. This loyalty card could tie to what they purchase at concessions and what movies they tend to see.

Topic 2 
Title: Experience
Top Word: Movie
Top 3 words: Movie, Great, Place

Strategy: I would utilize the information from the loyalty card program to segment which customers would want to watch what movie and then push adds to those people.  This is the first action we need our customers to take during the customer journey so ideally it will resort in concession stand sales. Our customers come for the experience and the movie so perhaps additional analysis could be done to see who they are sharing the experience with.  If it is a group of friends going maybe offer a discount for 6 or more people. If it is a date night maybe offer a date night package with two tickets, drinks, and popcorn.  A couple of these concepts could be tested to see what customers respond to.  If we can increase the average group size and the frequency of people going to the movies concessions should follow.  

Topic 3 
Title: Random Words
Top Word: Just
Top 3 words: Just, Like, Guest

Strategy: Nothing emerged out of this topic to act upon.  Perplexity for three topics was lower then two topics which means three topics was better for our LDA model.  However, although the perplexity score for three topics was better, I would have preferred to keep this at two topics since the third topic did not provide any real value.


```{r, warning=FALSE, message=FALSE}
library(reshape2)
library(tidyverse)
library(topicmodels)
library(ggplot2)
library(dplyr)
library(tidytext)
library(tidyr)
library(RTextTools)
library(wordcloud)
library(tm)
library(stringr)

 
## Topic model LDA 

reviews_LDA <- read.csv(url("http://data.mishra.us/files/project_reviews.csv"))

LDA_corpus <- VCorpus(VectorSource(reviews_LDA$text)) #create corpus

#Preprocessing

LDA_corpus <- tm_map(LDA_corpus, content_transformer(tolower)) # covert all to lower case else same word as lower and uppercase will classified as different
LDA_corpus <- tm_map(LDA_corpus, removeWords, stopwords(kind="en")) # remove stop words
func_Space <- content_transformer(function(x, pattern) gsub(pattern, " ", x)) # a function to clean /,@,\\,|
LDA_corpus <- tm_map(LDA_corpus, func_Space, "/|@|\\|")
LDA_corpus <- tm_map(LDA_corpus, stripWhitespace) # remove white space
LDA_corpus <- tm_map(LDA_corpus, removeNumbers) # remove numeric values
LDA_corpus <- tm_map(LDA_corpus, removePunctuation) # remove punctuation


LDA_dtm <- DocumentTermMatrix(LDA_corpus) #create a DTM


rowTotals <- apply(LDA_dtm , 1, sum) #sum of words
LDA_dtm   <- LDA_dtm [rowTotals> 0, ] # removed rows with no words 

set.seed(123)
lda_topic <- LDA(LDA_dtm, k = 3, method = "Gibbs", control = NULL) # 3 LDA topics
LDA_topic_model <- tidy(lda_topic, matrix = "beta") 

top_terms <- LDA_topic_model %>%
  group_by(topic) %>%
  top_n(10, beta) %>% # 10 terms
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


# perplexity calculation - change k = values
lda <- LDA(LDA_dtm, k = 2, control = NULL) 
perplexity(lda)

```
